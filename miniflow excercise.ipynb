{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 + 5  = 15 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "You need to change the Add() class below.\n",
    "\"\"\"\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Nodes from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Nodes to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # an Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    # NOTE: Input node is the only node that may\n",
    "    # receive its value as an argument to forward().\n",
    "    #\n",
    "    # All other node implementations should calculate their\n",
    "    # values from the value of previous nodes, using\n",
    "    # self.inbound_nodes\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def __init__(self, x, y):\n",
    "        # You could access `x` and `y` in forward with\n",
    "        # self.inbound_nodes[0] (`x`) and self.inbound_nodes[1] (`y`)\n",
    "        Node.__init__(self, [x, y])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node (`self.value`) to the sum of its inbound_nodes.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        x = self.inbound_nodes[0].value\n",
    "        y = self.inbound_nodes[1].value\n",
    "        self.value = x+y\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "No need to change anything below here!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n",
    "\n",
    "\n",
    "#from miniflow import *\n",
    "\n",
    "x, y, z = Input(), Input(), Input()\n",
    "\n",
    "f = Add(x, y)\n",
    "\n",
    "feed_dict = {x: 10, y: 5}\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "output = forward_pass(f, sorted_nodes)\n",
    "\n",
    "# NOTE: because topological_sort set the values for the `Input` nodes we could also access\n",
    "# the value for x with x.value (same goes for y).\n",
    "print(\"{} + {}  = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sum(*args):\n",
    "    sum =0\n",
    "    for i in args:\n",
    "        sum = sum +i\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Can you make Add accept any number of inputs? Eg. Add(x, y, z).\n",
    "* Can you make a Mul class that multiplies n inputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node: 4\n",
      "4\n",
      "forward  3\n",
      "0 4\n",
      "1 9\n",
      "2 19\n",
      "3 22\n",
      "4 + 5 + 10 + 3  = 22 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bonus Challenge!\n",
    "\n",
    "Write your code in Add (scroll down).\n",
    "\"\"\"\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Nodes from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Nodes to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # An Input Node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    # NOTE: Input Node is the only Node where the value\n",
    "    # may be passed as an argument to forward().\n",
    "    #\n",
    "    # All other Node implementations should get the value\n",
    "    # of the previous nodes from self.inbound_nodes\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Can you augment the Add class so that it accepts\n",
    "any number of nodes as input?\n",
    "\n",
    "Hint: this may be useful:\n",
    "https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists\n",
    "\"\"\"\n",
    "class Add(Node):\n",
    "    # You may need to change this...\n",
    "    def __init__(self, *inputs):\n",
    "        Node.__init__(self, inputs)\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        For reference, here's the old way from the last\n",
    "        quiz. You'll want to write code here.\n",
    "        \"\"\"\n",
    "        # x_value = self.inbound_nodes[0].value\n",
    "        # y_value = self.inbound_nodes[1].value\n",
    "        # self.value = x_value + y_value\n",
    "        \n",
    "        self.value =0\n",
    "        #print(len(self.inbound_nodes))\n",
    "        #print (\"forward \", L1)\n",
    "        for i in range(len(self.inbound_nodes)):\n",
    "            print(i,self.value+self.inbound_nodes[i].value)\n",
    "            self.value= self.value + self.inbound_nodes[i].value\n",
    "        #self.value= self.inbound_nodes[0].value +self.inbound_nodes[1].value+self.inbound_nodes[2].value\n",
    "           \n",
    "\n",
    "        \n",
    "        \n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n",
    "\n",
    "\n",
    "###===\n",
    "\"\"\"\n",
    "No need to change anything here!\n",
    "\n",
    "If all goes well, this should work after you\n",
    "modify the Add class in miniflow.py.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#from miniflow import *\n",
    "\n",
    "x, y, z ,a,b = Input(), Input(), Input() , Input(), Input()\n",
    "\n",
    "f = Add(x, y, z, a )\n",
    "\n",
    "feed_dict = {x: 4, y: 5,z:10, a:3 }\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "# should output 22\n",
    "print(\"{} + {} + {} + {}  = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y],\n",
    "                                                              feed_dict[z], feed_dict[a] ,output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "L1=3\n",
    "for i in range(L1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#from miniflow import *\n",
    "\n",
    "x, y, z ,a,b = Input(), Input(), Input() , Input(), Input()\n",
    "\n",
    "f = Add(x, y, z, a, b)\n",
    "\n",
    "feed_dict = {x: 4, y: 5,z:10, a:3, b:1}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "# should output 23\n",
    "print(\"{} + {} + {} + {} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y],\n",
    "                                                              feed_dict[z], feed_dict[a],\n",
    "                                                              feed_dict[b],output))\n",
    "\n",
    "\n",
    "#from miniflow import *\n",
    "\n",
    "x, y, z ,a,b = Input(), Input(), Input() , Input(), Input()\n",
    "\n",
    "f = Add(x, y, z, a )\n",
    "\n",
    "feed_dict = {x: 4, y: 5,z:10, a:3 }\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "# should output 22\n",
    "print(\"{} + {} + {} + {}  = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y],\n",
    "                                                              feed_dict[z], feed_dict[a] ,output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Write the Linear#forward method below!\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "class Node:\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Nodes from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Nodes to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # An Input Node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "        # NOTE: Input Node is the only Node where the value\n",
    "        # may be passed as an argument to forward().\n",
    "        #\n",
    "        # All other Node implementations should get the value\n",
    "        # of the previous nodes from self.inbound_nodes\n",
    "        #\n",
    "        # Example:\n",
    "        # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        Node.__init__(self, [inputs, weights, bias])\n",
    "\n",
    "        # NOTE: The weights and bias properties here are not\n",
    "        # numbers, but rather references to other nodes.\n",
    "        # The weight and bias values are stored within the\n",
    "        # respective nodes.\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set self.value to the value of the linear function output.\n",
    "\n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        inputs_val = self.inbound_nodes[0].value\n",
    "        weights_val = self.inbound_nodes[1].value\n",
    "        bias_val = self.inbound_nodes[2].value\n",
    "\n",
    "        self.value= 0\n",
    "        for x,y in zip(inputs_val, weights_val):\n",
    "            self.value += x*y\n",
    "        self.value += bias_val \n",
    "        \n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "\"\"\"\n",
    "NOTE: Here we're using an Input node for more than a scalar.\n",
    "In the case of weights and inputs the value of the Input node is\n",
    "actually a python list!\n",
    "\n",
    "In general, there's no restriction on the values that can be passed to an Input node.\n",
    "\"\"\"\n",
    "#from miniflow import *\n",
    "\n",
    "inputs, weights, bias = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "\n",
    "feed_dict = {\n",
    "    inputs: [6, 14, 3],\n",
    "    weights: [0.5, 0.25, 1.4],\n",
    "    bias: 2\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(output) # should be 12.7 with this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Linear Trasnform\n",
    "Instructions\n",
    "* Open nn.py. See how the neural network implements the Linear node.\n",
    "* Open miniflow.py. Implement Equation (2) within the forward pass for the Linear node.\n",
    "* Test your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.  4.]\n",
      " [-9.  4.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Modify Linear#forward so that it linearly transforms\n",
    "input matrices, weights matrices and a bias vector to\n",
    "an output.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        self.value = None\n",
    "        self.outbound_nodes = []\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward():\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    While it may be strange to consider an input a node when\n",
    "    an input is only an individual node in a node, for the sake\n",
    "    of simpler code we'll still use Node as the base class.\n",
    "\n",
    "    Think of Input as collating many individual input nodes into\n",
    "    a Node.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # An Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, X, W, b):\n",
    "        # Notice the ordering of the input nodes passed to the\n",
    "        # Node constructor.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node to the linear transform output.\n",
    "\n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        X_val = self.inbound_nodes[0].value\n",
    "        W_val = self.inbound_nodes[1].value\n",
    "        b_val = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X_val,W_val)  + b_val \n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A Node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: a topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n",
    "\n",
    "\"\"\"\n",
    "The setup is similar to the prevous `Linear` node you wrote\n",
    "except you're now using NumPy arrays instead of python lists.\n",
    "\n",
    "Update the Linear class in miniflow.py to work with\n",
    "numpy vectors (arrays) and matrices.\n",
    "\n",
    "Test your code here!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#from miniflow import *\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[-9., 4.],\n",
    "[-9., 4.]]\n",
    "\"\"\"\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Sigmoid Function\n",
    "Instructions\n",
    "* Open nn.py to see how the network will use Sigmoid.\n",
    "* Open miniflow.py. Modify the forward method of the Sigmoid class to reflect the sigmoid function's behavior.\n",
    "* Test your work! Hit \"Submit\" when your Sigmoid works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fix the Sigmoid class so that it computes the sigmoid function\n",
    "on the forward pass!\n",
    "\n",
    "Scroll down to get started.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        self.value = None\n",
    "        self.outbound_nodes = []\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward():\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # An Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, X, W, b):\n",
    "        # Notice the ordering of the input nodes passed to the\n",
    "        # Node constructor.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    You need to fix the `_sigmoid` and `forward` methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used later with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "\n",
    "        Return the result of the sigmoid function.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        #Node.__init__(self, x)\n",
    "        return 1. / (1. + np.exp(-x) )\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node to the result of the\n",
    "        sigmoid function, `_sigmoid`.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        # This is a dummy value to prevent numpy errors\n",
    "        # if you test without changing this method.\n",
    "        #self.value = -1\n",
    "        x = self.inbound_nodes[0].value\n",
    "        self.value =self._sigmoid(x)\n",
    "        \n",
    "        #x = 1. / (1. + np.exp(-self.inbound_nodes[0].value) )\n",
    "        #self.value = x\n",
    "\n",
    "#·=====or\n",
    "\n",
    "#class Sigmoid(Node):\n",
    "#    def __init__(self, node):\n",
    "#        Node.__init__(self, [node])\n",
    "#\n",
    "#    def _sigmoid(self, x):\n",
    "#        \"\"\"\n",
    "#        This method is separate from `forward` because it\n",
    "#        will be used with `backward` as well.\n",
    "\n",
    "#        `x`: A numpy array-like object.\n",
    "#        \"\"\"\n",
    "#        return 1. / (1. + np.exp(-x)) # the `.` ensures that `1` is a float\n",
    "#\n",
    "#    def forward(self):\n",
    "#        input_value = self.inbound_nodes[0].value\n",
    "#        self.value = self._sigmoid(input_value)\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A Node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: a topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n",
    "\n",
    "\"\"\"\n",
    "This network feeds the output of a linear transform\n",
    "to the sigmoid function.\n",
    "\n",
    "Finish implementing the Sigmoid class in miniflow.py!\n",
    "\n",
    "Feel free to play around with this network, too!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "#from miniflow import *\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(g, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  1.23394576e-04   9.82013790e-01]\n",
    " [  1.23394576e-04   9.82013790e-01]]\n",
    "\"\"\"\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cost\n",
    "Instructions\n",
    "For this quiz, you will run the forward pass against the network in nn.py. I want you to finish implementing the MSE method so that it calculates the cost from the equation above.\n",
    "\n",
    "I recommend using the np.square (documentation) method to make your life easier.\n",
    "\n",
    "* Check out nn.py to see how MSE will calculate the cost.\n",
    "* Open miniflow.py. Finish building MSE.\n",
    "* Test your network! See if the cost makes sense given the inputs by playing with nn.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[1 2 3 4]\n",
      "[1 2 3 4 5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "# 2 by 2 matrices\n",
    "w1  = np.array([[1, 2], [3, 4]])\n",
    "w2  = np.array([[5, 6], [7, 8]])\n",
    "print (w1)\n",
    "\n",
    "# flatten\n",
    "w1_flat = np.reshape(w1, -1)\n",
    "w2_flat = np.reshape(w2, -1)\n",
    "print(w1_flat)\n",
    "\n",
    "w = np.concatenate((w1_flat, w2_flat))\n",
    "# array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "3\n",
      "23.4166666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        m = self.inbound_nodes[0].value\n",
    "        print(m)\n",
    "        m = self.inbound_nodes[0].value.shape[0]\n",
    "        print(m)\n",
    "\n",
    "        # TODO: your code here\n",
    "        \n",
    "        #self.value = np.mean(np.square(y-a))\n",
    "        self.value =(1 /m) * np.sum(np.square(y-a))\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "Test your MSE method with this script!\n",
    "\n",
    "No changes necessary, but feel free to play\n",
    "with this script to test your network.\n",
    "\"\"\"\n",
    "\n",
    "#import numpy as np\n",
    "#from miniflow import *\n",
    "\n",
    "y, a = Input(), Input()\n",
    "cost = MSE(y, a)\n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, a: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_pass(graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journey to the Bottom of the Valley\n",
    "Setup\n",
    "For this quiz you'll complete TODOs in both the f.py and gd.py files.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Set the learning_rate in f.py.\n",
    "Complete the gradient descent implementation in gradient_descent_update function in gd.py.\n",
    "Notes:\n",
    "\n",
    "Setting the learning_rate to 0.1 should result in x -> 0 and f(x) -> 5 if you've implemented gradient descent correctly.\n",
    "\n",
    "Play around with different values for the learning rate. Try very small values, values close to 1, above 1, etc. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0: Cost = 519846.000, x = 1442.000\n",
      "EPOCH 1: Cost = 349546.088, x = 1182.440\n",
      "EPOCH 2: Cost = 235036.428, x = 969.601\n",
      "EPOCH 3: Cost = 158040.132, x = 795.073\n",
      "EPOCH 4: Cost = 106267.823, x = 651.960\n",
      "EPOCH 5: Cost = 71456.122, x = 534.607\n",
      "EPOCH 6: Cost = 48048.734, x = 438.378\n",
      "EPOCH 7: Cost = 32309.607, x = 359.470\n",
      "EPOCH 8: Cost = 21726.618, x = 294.765\n",
      "EPOCH 9: Cost = 14610.616, x = 241.707\n",
      "EPOCH 10: Cost = 9825.816, x = 198.200\n",
      "EPOCH 11: Cost = 6608.517, x = 162.524\n",
      "EPOCH 12: Cost = 4445.205, x = 133.270\n",
      "EPOCH 13: Cost = 2990.594, x = 109.281\n",
      "EPOCH 14: Cost = 2012.513, x = 89.611\n",
      "EPOCH 15: Cost = 1354.852, x = 73.481\n",
      "EPOCH 16: Cost = 912.640, x = 60.254\n",
      "EPOCH 17: Cost = 615.297, x = 49.408\n",
      "EPOCH 18: Cost = 415.364, x = 40.515\n",
      "EPOCH 19: Cost = 280.929, x = 33.222\n",
      "EPOCH 20: Cost = 190.534, x = 27.242\n",
      "EPOCH 21: Cost = 129.753, x = 22.339\n",
      "EPOCH 22: Cost = 88.884, x = 18.318\n",
      "EPOCH 23: Cost = 61.404, x = 15.020\n",
      "EPOCH 24: Cost = 42.926, x = 12.317\n",
      "EPOCH 25: Cost = 30.501, x = 10.100\n",
      "EPOCH 26: Cost = 22.147, x = 8.282\n",
      "EPOCH 27: Cost = 16.530, x = 6.791\n",
      "EPOCH 28: Cost = 12.753, x = 5.569\n",
      "EPOCH 29: Cost = 10.213, x = 4.566\n",
      "EPOCH 30: Cost = 8.505, x = 3.744\n",
      "EPOCH 31: Cost = 7.357, x = 3.070\n",
      "EPOCH 32: Cost = 6.585, x = 2.518\n",
      "EPOCH 33: Cost = 6.066, x = 2.065\n",
      "EPOCH 34: Cost = 5.716, x = 1.693\n",
      "EPOCH 35: Cost = 5.482, x = 1.388\n",
      "EPOCH 36: Cost = 5.324, x = 1.138\n",
      "EPOCH 37: Cost = 5.218, x = 0.933\n",
      "EPOCH 38: Cost = 5.146, x = 0.765\n",
      "EPOCH 39: Cost = 5.098, x = 0.628\n",
      "EPOCH 40: Cost = 5.066, x = 0.515\n",
      "EPOCH 41: Cost = 5.045, x = 0.422\n",
      "EPOCH 42: Cost = 5.030, x = 0.346\n",
      "EPOCH 43: Cost = 5.020, x = 0.284\n",
      "EPOCH 44: Cost = 5.014, x = 0.233\n",
      "EPOCH 45: Cost = 5.009, x = 0.191\n",
      "EPOCH 46: Cost = 5.006, x = 0.156\n",
      "EPOCH 47: Cost = 5.004, x = 0.128\n",
      "EPOCH 48: Cost = 5.003, x = 0.105\n",
      "EPOCH 49: Cost = 5.002, x = 0.086\n",
      "EPOCH 50: Cost = 5.001, x = 0.071\n",
      "EPOCH 51: Cost = 5.001, x = 0.058\n",
      "EPOCH 52: Cost = 5.001, x = 0.048\n",
      "EPOCH 53: Cost = 5.000, x = 0.039\n",
      "EPOCH 54: Cost = 5.000, x = 0.032\n",
      "EPOCH 55: Cost = 5.000, x = 0.026\n",
      "EPOCH 56: Cost = 5.000, x = 0.022\n",
      "EPOCH 57: Cost = 5.000, x = 0.018\n",
      "EPOCH 58: Cost = 5.000, x = 0.014\n",
      "EPOCH 59: Cost = 5.000, x = 0.012\n",
      "EPOCH 60: Cost = 5.000, x = 0.010\n",
      "EPOCH 61: Cost = 5.000, x = 0.008\n",
      "EPOCH 62: Cost = 5.000, x = 0.007\n",
      "EPOCH 63: Cost = 5.000, x = 0.005\n",
      "EPOCH 64: Cost = 5.000, x = 0.004\n",
      "EPOCH 65: Cost = 5.000, x = 0.004\n",
      "EPOCH 66: Cost = 5.000, x = 0.003\n",
      "EPOCH 67: Cost = 5.000, x = 0.002\n",
      "EPOCH 68: Cost = 5.000, x = 0.002\n",
      "EPOCH 69: Cost = 5.000, x = 0.002\n",
      "EPOCH 70: Cost = 5.000, x = 0.001\n",
      "EPOCH 71: Cost = 5.000, x = 0.001\n",
      "EPOCH 72: Cost = 5.000, x = 0.001\n",
      "EPOCH 73: Cost = 5.000, x = 0.001\n",
      "EPOCH 74: Cost = 5.000, x = 0.001\n",
      "EPOCH 75: Cost = 5.000, x = 0.000\n",
      "EPOCH 76: Cost = 5.000, x = 0.000\n",
      "EPOCH 77: Cost = 5.000, x = 0.000\n",
      "EPOCH 78: Cost = 5.000, x = 0.000\n",
      "EPOCH 79: Cost = 5.000, x = 0.000\n",
      "EPOCH 80: Cost = 5.000, x = 0.000\n",
      "EPOCH 81: Cost = 5.000, x = 0.000\n",
      "EPOCH 82: Cost = 5.000, x = 0.000\n",
      "EPOCH 83: Cost = 5.000, x = 0.000\n",
      "EPOCH 84: Cost = 5.000, x = 0.000\n",
      "EPOCH 85: Cost = 5.000, x = 0.000\n",
      "EPOCH 86: Cost = 5.000, x = 0.000\n",
      "EPOCH 87: Cost = 5.000, x = 0.000\n",
      "EPOCH 88: Cost = 5.000, x = 0.000\n",
      "EPOCH 89: Cost = 5.000, x = 0.000\n",
      "EPOCH 90: Cost = 5.000, x = 0.000\n",
      "EPOCH 91: Cost = 5.000, x = 0.000\n",
      "EPOCH 92: Cost = 5.000, x = 0.000\n",
      "EPOCH 93: Cost = 5.000, x = 0.000\n",
      "EPOCH 94: Cost = 5.000, x = 0.000\n",
      "EPOCH 95: Cost = 5.000, x = 0.000\n",
      "EPOCH 96: Cost = 5.000, x = 0.000\n",
      "EPOCH 97: Cost = 5.000, x = 0.000\n",
      "EPOCH 98: Cost = 5.000, x = 0.000\n",
      "EPOCH 99: Cost = 5.000, x = 0.000\n",
      "EPOCH 100: Cost = 5.000, x = 0.000\n"
     ]
    }
   ],
   "source": [
    "#gd.py\n",
    "def gradient_descent_update(x, gradx, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a gradient descent update.\n",
    "    \"\"\"\n",
    "    # TODO: Implement gradient descent.\n",
    "    x = x - learning_rate * gradx\n",
    "    # Return the new value for x\n",
    "    return x\n",
    "\n",
    "#f.py\n",
    "\"\"\"\n",
    "Given the starting point of any `x` gradient descent\n",
    "should be able to find the minimum value of x for the\n",
    "cost function `f` defined below.\n",
    "\"\"\"\n",
    "import random\n",
    "#from gd import gradient_descent_update\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Quadratic function.\n",
    "\n",
    "    It's easy to see the minimum value of the function\n",
    "    is 5 when is x=0.\n",
    "    \"\"\"\n",
    "    return x**2 + 5\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"\n",
    "    Derivative of `f` with respect to `x`.\n",
    "    \"\"\"\n",
    "    return 2*x\n",
    "\n",
    "\n",
    "# Random number better 0 and 10,000. Feel free to set x whatever you like.\n",
    "x = random.randint(0, 10000)\n",
    "# TODO: Set the learning rate\n",
    "learning_rate = .09\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    cost = f(x)\n",
    "    gradx = df(x)\n",
    "    print(\"EPOCH {}: Cost = {:.3f}, x = {:.3f}\".format(i, cost, gradx))\n",
    "    x = gradient_descent_update(x, gradx, learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Gradient & Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "Here's the derivative of the sigmoid function w.r.t x:\n",
    "\n",
    "sigmoid(x)=1/(1+exp(−x))\n",
    "\n",
    "∂x/∂sigmoid =sigmoid(x)∗(1−sigmoid(x))\n",
    "\n",
    "Complete the implementation of backpropagation for the Sigmoid node by finishing the backward method in miniflow.py.\n",
    "The backward methods for all other nodes have already been implemented. Taking a look at them might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
      "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
      "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
      "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implement the backward method of the Sigmoid node.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] += grad_cost * 1\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            \"\"\"\n",
    "            TODO: Your code goes here!\n",
    "\n",
    "            Set the gradients property to the gradients with respect to each input.\n",
    "\n",
    "            NOTE: See the Linear node and MSE node for examples.\n",
    "            \"\"\"\n",
    "            #sigmoid(x)∗(1−sigmoid(x))\n",
    "            sigmoid =self.value\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1- sigmoid) * grad_cost\n",
    "\n",
    "            \n",
    "            \n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "\n",
    "        This is the final node of the network so outbound nodes\n",
    "        are not a concern.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "####------\n",
    "\"\"\"\n",
    "Test your network here!\n",
    "\n",
    "No need to change this code, but feel free to tweak it\n",
    "to test your network!\n",
    "\n",
    "Make your changes to backward method of the Sigmoid class in miniflow.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "#from miniflow import *\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "y = Input()\n",
    "f = Linear(X, W, b)\n",
    "a = Sigmoid(f)\n",
    "cost = MSE(y, a)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2.], [3.]])\n",
    "b_ = np.array([-3.])\n",
    "y_ = np.array([1, 2])\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W: W_,\n",
    "    b: b_,\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_and_backward(graph)\n",
    "# return the gradients for each Input\n",
    "gradients = [t.gradients[t] for t in [X, y, W, b]]\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
    "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
    "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
    "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n",
    "\"\"\"\n",
    "print(gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Stochastic Gradient Descent\n",
    "Instructions\n",
    "* Open nn.py. See how the network runs with this new architecture.\n",
    "* Find the sgd_update method in miniflow.py and implement SGD.\n",
    "* Test your network! Does your loss decrease with more epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 114.562\n",
      "Epoch: 2, Loss: 32.939\n",
      "Epoch: 3, Loss: 27.955\n",
      "Epoch: 4, Loss: 25.119\n",
      "Epoch: 5, Loss: 23.319\n",
      "Epoch: 6, Loss: 19.626\n",
      "Epoch: 7, Loss: 22.149\n",
      "Epoch: 8, Loss: 16.915\n",
      "Epoch: 9, Loss: 18.479\n",
      "Epoch: 10, Loss: 16.973\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            self.gradients[self] += n.gradients[self]\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Sum the partial with respect to the input over all the outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` Nodes representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    # TODO: update all the `trainables` with SGD\n",
    "    # You can access and assign the value of a trainable with `value` attribute.\n",
    "    # Example:\n",
    "    for t in trainables:\n",
    "    #   t.value = your implementation here\n",
    "        #partial =\n",
    "        #x=x−α∗∂x.∂cost \n",
    "        t.value = t.value- learning_rate *  t.gradients[t]\n",
    "    \n",
    "\n",
    "\n",
    "##-------\n",
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "#import numpy as np\n",
    "#from sklearn.datasets import load_boston\n",
    "#from sklearn.utils import shuffle, resample\n",
    "#from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 10\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to 1000\n",
    "I'm putting the same quiz below again. If you haven't already, set the number of epochs to something like 1000 and watch as the loss decreases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 113.366\n",
      "Epoch: 2, Loss: 31.255\n",
      "Epoch: 3, Loss: 25.421\n",
      "Epoch: 4, Loss: 23.242\n",
      "Epoch: 5, Loss: 21.915\n",
      "Epoch: 6, Loss: 20.043\n",
      "Epoch: 7, Loss: 17.064\n",
      "Epoch: 8, Loss: 18.227\n",
      "Epoch: 9, Loss: 19.617\n",
      "Epoch: 10, Loss: 13.686\n",
      "Epoch: 11, Loss: 13.950\n",
      "Epoch: 12, Loss: 15.776\n",
      "Epoch: 13, Loss: 12.863\n",
      "Epoch: 14, Loss: 10.989\n",
      "Epoch: 15, Loss: 9.229\n",
      "Epoch: 16, Loss: 13.020\n",
      "Epoch: 17, Loss: 10.740\n",
      "Epoch: 18, Loss: 10.612\n",
      "Epoch: 19, Loss: 8.246\n",
      "Epoch: 20, Loss: 10.024\n",
      "Epoch: 21, Loss: 10.429\n",
      "Epoch: 22, Loss: 11.622\n",
      "Epoch: 23, Loss: 10.011\n",
      "Epoch: 24, Loss: 10.537\n",
      "Epoch: 25, Loss: 11.553\n",
      "Epoch: 26, Loss: 11.635\n",
      "Epoch: 27, Loss: 10.895\n",
      "Epoch: 28, Loss: 11.964\n",
      "Epoch: 29, Loss: 8.221\n",
      "Epoch: 30, Loss: 12.508\n",
      "Epoch: 31, Loss: 9.970\n",
      "Epoch: 32, Loss: 12.833\n",
      "Epoch: 33, Loss: 8.611\n",
      "Epoch: 34, Loss: 8.727\n",
      "Epoch: 35, Loss: 9.658\n",
      "Epoch: 36, Loss: 7.640\n",
      "Epoch: 37, Loss: 10.025\n",
      "Epoch: 38, Loss: 9.992\n",
      "Epoch: 39, Loss: 8.952\n",
      "Epoch: 40, Loss: 10.910\n",
      "Epoch: 41, Loss: 8.666\n",
      "Epoch: 42, Loss: 7.733\n",
      "Epoch: 43, Loss: 9.665\n",
      "Epoch: 44, Loss: 8.423\n",
      "Epoch: 45, Loss: 11.250\n",
      "Epoch: 46, Loss: 10.352\n",
      "Epoch: 47, Loss: 9.858\n",
      "Epoch: 48, Loss: 7.722\n",
      "Epoch: 49, Loss: 8.967\n",
      "Epoch: 50, Loss: 8.306\n",
      "Epoch: 51, Loss: 6.836\n",
      "Epoch: 52, Loss: 8.598\n",
      "Epoch: 53, Loss: 7.827\n",
      "Epoch: 54, Loss: 7.647\n",
      "Epoch: 55, Loss: 8.748\n",
      "Epoch: 56, Loss: 7.780\n",
      "Epoch: 57, Loss: 9.058\n",
      "Epoch: 58, Loss: 8.138\n",
      "Epoch: 59, Loss: 8.871\n",
      "Epoch: 60, Loss: 8.687\n",
      "Epoch: 61, Loss: 7.894\n",
      "Epoch: 62, Loss: 6.996\n",
      "Epoch: 63, Loss: 7.939\n",
      "Epoch: 64, Loss: 8.877\n",
      "Epoch: 65, Loss: 6.656\n",
      "Epoch: 66, Loss: 6.964\n",
      "Epoch: 67, Loss: 7.637\n",
      "Epoch: 68, Loss: 7.651\n",
      "Epoch: 69, Loss: 7.165\n",
      "Epoch: 70, Loss: 7.481\n",
      "Epoch: 71, Loss: 7.129\n",
      "Epoch: 72, Loss: 7.330\n",
      "Epoch: 73, Loss: 8.153\n",
      "Epoch: 74, Loss: 7.240\n",
      "Epoch: 75, Loss: 7.802\n",
      "Epoch: 76, Loss: 7.144\n",
      "Epoch: 77, Loss: 8.285\n",
      "Epoch: 78, Loss: 7.111\n",
      "Epoch: 79, Loss: 8.507\n",
      "Epoch: 80, Loss: 7.784\n",
      "Epoch: 81, Loss: 7.320\n",
      "Epoch: 82, Loss: 8.093\n",
      "Epoch: 83, Loss: 6.298\n",
      "Epoch: 84, Loss: 7.231\n",
      "Epoch: 85, Loss: 6.685\n",
      "Epoch: 86, Loss: 7.932\n",
      "Epoch: 87, Loss: 7.903\n",
      "Epoch: 88, Loss: 7.023\n",
      "Epoch: 89, Loss: 7.712\n",
      "Epoch: 90, Loss: 6.861\n",
      "Epoch: 91, Loss: 5.409\n",
      "Epoch: 92, Loss: 7.806\n",
      "Epoch: 93, Loss: 5.947\n",
      "Epoch: 94, Loss: 6.611\n",
      "Epoch: 95, Loss: 6.418\n",
      "Epoch: 96, Loss: 6.720\n",
      "Epoch: 97, Loss: 6.411\n",
      "Epoch: 98, Loss: 7.864\n",
      "Epoch: 99, Loss: 5.166\n",
      "Epoch: 100, Loss: 6.414\n",
      "Epoch: 101, Loss: 6.846\n",
      "Epoch: 102, Loss: 7.105\n",
      "Epoch: 103, Loss: 7.352\n",
      "Epoch: 104, Loss: 6.518\n",
      "Epoch: 105, Loss: 8.131\n",
      "Epoch: 106, Loss: 6.663\n",
      "Epoch: 107, Loss: 6.670\n",
      "Epoch: 108, Loss: 7.951\n",
      "Epoch: 109, Loss: 6.300\n",
      "Epoch: 110, Loss: 7.403\n",
      "Epoch: 111, Loss: 6.158\n",
      "Epoch: 112, Loss: 7.001\n",
      "Epoch: 113, Loss: 5.971\n",
      "Epoch: 114, Loss: 6.336\n",
      "Epoch: 115, Loss: 8.258\n",
      "Epoch: 116, Loss: 5.758\n",
      "Epoch: 117, Loss: 6.586\n",
      "Epoch: 118, Loss: 6.064\n",
      "Epoch: 119, Loss: 6.474\n",
      "Epoch: 120, Loss: 8.102\n",
      "Epoch: 121, Loss: 6.509\n",
      "Epoch: 122, Loss: 6.053\n",
      "Epoch: 123, Loss: 6.026\n",
      "Epoch: 124, Loss: 8.085\n",
      "Epoch: 125, Loss: 6.186\n",
      "Epoch: 126, Loss: 6.224\n",
      "Epoch: 127, Loss: 7.644\n",
      "Epoch: 128, Loss: 7.107\n",
      "Epoch: 129, Loss: 7.223\n",
      "Epoch: 130, Loss: 6.557\n",
      "Epoch: 131, Loss: 7.861\n",
      "Epoch: 132, Loss: 6.891\n",
      "Epoch: 133, Loss: 7.266\n",
      "Epoch: 134, Loss: 7.111\n",
      "Epoch: 135, Loss: 6.521\n",
      "Epoch: 136, Loss: 6.755\n",
      "Epoch: 137, Loss: 5.951\n",
      "Epoch: 138, Loss: 6.710\n",
      "Epoch: 139, Loss: 6.027\n",
      "Epoch: 140, Loss: 6.580\n",
      "Epoch: 141, Loss: 6.804\n",
      "Epoch: 142, Loss: 6.050\n",
      "Epoch: 143, Loss: 7.385\n",
      "Epoch: 144, Loss: 5.700\n",
      "Epoch: 145, Loss: 6.299\n",
      "Epoch: 146, Loss: 6.028\n",
      "Epoch: 147, Loss: 7.051\n",
      "Epoch: 148, Loss: 6.641\n",
      "Epoch: 149, Loss: 6.245\n",
      "Epoch: 150, Loss: 6.185\n",
      "Epoch: 151, Loss: 5.037\n",
      "Epoch: 152, Loss: 5.687\n",
      "Epoch: 153, Loss: 6.681\n",
      "Epoch: 154, Loss: 5.780\n",
      "Epoch: 155, Loss: 4.819\n",
      "Epoch: 156, Loss: 5.854\n",
      "Epoch: 157, Loss: 5.610\n",
      "Epoch: 158, Loss: 6.200\n",
      "Epoch: 159, Loss: 6.102\n",
      "Epoch: 160, Loss: 7.571\n",
      "Epoch: 161, Loss: 5.783\n",
      "Epoch: 162, Loss: 5.943\n",
      "Epoch: 163, Loss: 6.218\n",
      "Epoch: 164, Loss: 5.377\n",
      "Epoch: 165, Loss: 5.633\n",
      "Epoch: 166, Loss: 5.706\n",
      "Epoch: 167, Loss: 5.780\n",
      "Epoch: 168, Loss: 5.522\n",
      "Epoch: 169, Loss: 5.205\n",
      "Epoch: 170, Loss: 6.522\n",
      "Epoch: 171, Loss: 6.077\n",
      "Epoch: 172, Loss: 5.712\n",
      "Epoch: 173, Loss: 5.406\n",
      "Epoch: 174, Loss: 5.880\n",
      "Epoch: 175, Loss: 4.455\n",
      "Epoch: 176, Loss: 5.167\n",
      "Epoch: 177, Loss: 6.032\n",
      "Epoch: 178, Loss: 5.265\n",
      "Epoch: 179, Loss: 6.262\n",
      "Epoch: 180, Loss: 5.271\n",
      "Epoch: 181, Loss: 5.422\n",
      "Epoch: 182, Loss: 5.642\n",
      "Epoch: 183, Loss: 6.231\n",
      "Epoch: 184, Loss: 5.039\n",
      "Epoch: 185, Loss: 5.698\n",
      "Epoch: 186, Loss: 5.441\n",
      "Epoch: 187, Loss: 5.506\n",
      "Epoch: 188, Loss: 4.418\n",
      "Epoch: 189, Loss: 5.796\n",
      "Epoch: 190, Loss: 5.683\n",
      "Epoch: 191, Loss: 5.110\n",
      "Epoch: 192, Loss: 6.577\n",
      "Epoch: 193, Loss: 5.330\n",
      "Epoch: 194, Loss: 6.309\n",
      "Epoch: 195, Loss: 6.250\n",
      "Epoch: 196, Loss: 6.012\n",
      "Epoch: 197, Loss: 4.830\n",
      "Epoch: 198, Loss: 5.711\n",
      "Epoch: 199, Loss: 5.236\n",
      "Epoch: 200, Loss: 5.886\n",
      "Epoch: 201, Loss: 5.471\n",
      "Epoch: 202, Loss: 5.522\n",
      "Epoch: 203, Loss: 5.271\n",
      "Epoch: 204, Loss: 5.362\n",
      "Epoch: 205, Loss: 6.207\n",
      "Epoch: 206, Loss: 5.199\n",
      "Epoch: 207, Loss: 5.152\n",
      "Epoch: 208, Loss: 6.688\n",
      "Epoch: 209, Loss: 4.602\n",
      "Epoch: 210, Loss: 5.229\n",
      "Epoch: 211, Loss: 5.338\n",
      "Epoch: 212, Loss: 4.985\n",
      "Epoch: 213, Loss: 5.768\n",
      "Epoch: 214, Loss: 5.132\n",
      "Epoch: 215, Loss: 4.906\n",
      "Epoch: 216, Loss: 5.126\n",
      "Epoch: 217, Loss: 5.928\n",
      "Epoch: 218, Loss: 4.512\n",
      "Epoch: 219, Loss: 5.298\n",
      "Epoch: 220, Loss: 6.279\n",
      "Epoch: 221, Loss: 5.736\n",
      "Epoch: 222, Loss: 5.700\n",
      "Epoch: 223, Loss: 4.138\n",
      "Epoch: 224, Loss: 5.198\n",
      "Epoch: 225, Loss: 4.241\n",
      "Epoch: 226, Loss: 4.417\n",
      "Epoch: 227, Loss: 4.482\n",
      "Epoch: 228, Loss: 4.777\n",
      "Epoch: 229, Loss: 6.141\n",
      "Epoch: 230, Loss: 5.128\n",
      "Epoch: 231, Loss: 5.311\n",
      "Epoch: 232, Loss: 4.974\n",
      "Epoch: 233, Loss: 5.530\n",
      "Epoch: 234, Loss: 5.424\n",
      "Epoch: 235, Loss: 5.297\n",
      "Epoch: 236, Loss: 5.329\n",
      "Epoch: 237, Loss: 5.002\n",
      "Epoch: 238, Loss: 5.391\n",
      "Epoch: 239, Loss: 4.909\n",
      "Epoch: 240, Loss: 4.500\n",
      "Epoch: 241, Loss: 5.119\n",
      "Epoch: 242, Loss: 5.886\n",
      "Epoch: 243, Loss: 6.026\n",
      "Epoch: 244, Loss: 4.554\n",
      "Epoch: 245, Loss: 4.690\n",
      "Epoch: 246, Loss: 4.588\n",
      "Epoch: 247, Loss: 4.545\n",
      "Epoch: 248, Loss: 5.551\n",
      "Epoch: 249, Loss: 3.958\n",
      "Epoch: 250, Loss: 4.817\n",
      "Epoch: 251, Loss: 4.931\n",
      "Epoch: 252, Loss: 5.439\n",
      "Epoch: 253, Loss: 5.320\n",
      "Epoch: 254, Loss: 5.073\n",
      "Epoch: 255, Loss: 5.230\n",
      "Epoch: 256, Loss: 5.420\n",
      "Epoch: 257, Loss: 4.429\n",
      "Epoch: 258, Loss: 4.403\n",
      "Epoch: 259, Loss: 4.867\n",
      "Epoch: 260, Loss: 4.715\n",
      "Epoch: 261, Loss: 5.040\n",
      "Epoch: 262, Loss: 5.229\n",
      "Epoch: 263, Loss: 5.565\n",
      "Epoch: 264, Loss: 4.943\n",
      "Epoch: 265, Loss: 5.145\n",
      "Epoch: 266, Loss: 4.701\n",
      "Epoch: 267, Loss: 4.767\n",
      "Epoch: 268, Loss: 4.653\n",
      "Epoch: 269, Loss: 3.871\n",
      "Epoch: 270, Loss: 5.810\n",
      "Epoch: 271, Loss: 4.954\n",
      "Epoch: 272, Loss: 5.738\n",
      "Epoch: 273, Loss: 4.743\n",
      "Epoch: 274, Loss: 4.691\n",
      "Epoch: 275, Loss: 4.407\n",
      "Epoch: 276, Loss: 4.984\n",
      "Epoch: 277, Loss: 4.499\n",
      "Epoch: 278, Loss: 4.691\n",
      "Epoch: 279, Loss: 5.943\n",
      "Epoch: 280, Loss: 4.851\n",
      "Epoch: 281, Loss: 5.511\n",
      "Epoch: 282, Loss: 5.012\n",
      "Epoch: 283, Loss: 4.207\n",
      "Epoch: 284, Loss: 4.814\n",
      "Epoch: 285, Loss: 5.295\n",
      "Epoch: 286, Loss: 5.156\n",
      "Epoch: 287, Loss: 4.233\n",
      "Epoch: 288, Loss: 4.698\n",
      "Epoch: 289, Loss: 5.230\n",
      "Epoch: 290, Loss: 5.547\n",
      "Epoch: 291, Loss: 6.081\n",
      "Epoch: 292, Loss: 5.013\n",
      "Epoch: 293, Loss: 4.871\n",
      "Epoch: 294, Loss: 4.931\n",
      "Epoch: 295, Loss: 5.148\n",
      "Epoch: 296, Loss: 4.558\n",
      "Epoch: 297, Loss: 5.364\n",
      "Epoch: 298, Loss: 4.887\n",
      "Epoch: 299, Loss: 4.854\n",
      "Epoch: 300, Loss: 5.108\n",
      "Epoch: 301, Loss: 4.067\n",
      "Epoch: 302, Loss: 4.910\n",
      "Epoch: 303, Loss: 4.976\n",
      "Epoch: 304, Loss: 4.371\n",
      "Epoch: 305, Loss: 4.143\n",
      "Epoch: 306, Loss: 4.489\n",
      "Epoch: 307, Loss: 4.183\n",
      "Epoch: 308, Loss: 4.522\n",
      "Epoch: 309, Loss: 4.755\n",
      "Epoch: 310, Loss: 4.544\n",
      "Epoch: 311, Loss: 5.250\n",
      "Epoch: 312, Loss: 4.248\n",
      "Epoch: 313, Loss: 5.178\n",
      "Epoch: 314, Loss: 4.307\n",
      "Epoch: 315, Loss: 4.375\n",
      "Epoch: 316, Loss: 4.520\n",
      "Epoch: 317, Loss: 4.424\n",
      "Epoch: 318, Loss: 4.382\n",
      "Epoch: 319, Loss: 4.600\n",
      "Epoch: 320, Loss: 4.857\n",
      "Epoch: 321, Loss: 4.766\n",
      "Epoch: 322, Loss: 4.733\n",
      "Epoch: 323, Loss: 5.339\n",
      "Epoch: 324, Loss: 4.029\n",
      "Epoch: 325, Loss: 4.504\n",
      "Epoch: 326, Loss: 4.556\n",
      "Epoch: 327, Loss: 4.945\n",
      "Epoch: 328, Loss: 4.721\n",
      "Epoch: 329, Loss: 5.223\n",
      "Epoch: 330, Loss: 4.243\n",
      "Epoch: 331, Loss: 4.312\n",
      "Epoch: 332, Loss: 4.355\n",
      "Epoch: 333, Loss: 4.888\n",
      "Epoch: 334, Loss: 4.942\n",
      "Epoch: 335, Loss: 4.547\n",
      "Epoch: 336, Loss: 5.061\n",
      "Epoch: 337, Loss: 4.519\n",
      "Epoch: 338, Loss: 4.447\n",
      "Epoch: 339, Loss: 4.397\n",
      "Epoch: 340, Loss: 5.181\n",
      "Epoch: 341, Loss: 4.296\n",
      "Epoch: 342, Loss: 4.666\n",
      "Epoch: 343, Loss: 4.877\n",
      "Epoch: 344, Loss: 4.003\n",
      "Epoch: 345, Loss: 5.235\n",
      "Epoch: 346, Loss: 4.544\n",
      "Epoch: 347, Loss: 4.785\n",
      "Epoch: 348, Loss: 4.096\n",
      "Epoch: 349, Loss: 4.654\n",
      "Epoch: 350, Loss: 4.964\n",
      "Epoch: 351, Loss: 4.351\n",
      "Epoch: 352, Loss: 5.105\n",
      "Epoch: 353, Loss: 4.418\n",
      "Epoch: 354, Loss: 4.284\n",
      "Epoch: 355, Loss: 4.623\n",
      "Epoch: 356, Loss: 4.200\n",
      "Epoch: 357, Loss: 4.907\n",
      "Epoch: 358, Loss: 4.215\n",
      "Epoch: 359, Loss: 5.129\n",
      "Epoch: 360, Loss: 4.461\n",
      "Epoch: 361, Loss: 4.787\n",
      "Epoch: 362, Loss: 4.025\n",
      "Epoch: 363, Loss: 4.641\n",
      "Epoch: 364, Loss: 4.751\n",
      "Epoch: 365, Loss: 4.524\n",
      "Epoch: 366, Loss: 4.382\n",
      "Epoch: 367, Loss: 4.633\n",
      "Epoch: 368, Loss: 4.487\n",
      "Epoch: 369, Loss: 3.841\n",
      "Epoch: 370, Loss: 4.436\n",
      "Epoch: 371, Loss: 5.486\n",
      "Epoch: 372, Loss: 4.806\n",
      "Epoch: 373, Loss: 4.253\n",
      "Epoch: 374, Loss: 4.491\n",
      "Epoch: 375, Loss: 4.257\n",
      "Epoch: 376, Loss: 5.050\n",
      "Epoch: 377, Loss: 5.434\n",
      "Epoch: 378, Loss: 4.274\n",
      "Epoch: 379, Loss: 4.735\n",
      "Epoch: 380, Loss: 4.054\n",
      "Epoch: 381, Loss: 4.889\n",
      "Epoch: 382, Loss: 4.993\n",
      "Epoch: 383, Loss: 4.748\n",
      "Epoch: 384, Loss: 4.159\n",
      "Epoch: 385, Loss: 4.445\n",
      "Epoch: 386, Loss: 4.099\n",
      "Epoch: 387, Loss: 4.613\n",
      "Epoch: 388, Loss: 3.980\n",
      "Epoch: 389, Loss: 5.434\n",
      "Epoch: 390, Loss: 4.667\n",
      "Epoch: 391, Loss: 4.210\n",
      "Epoch: 392, Loss: 4.931\n",
      "Epoch: 393, Loss: 4.936\n",
      "Epoch: 394, Loss: 4.289\n",
      "Epoch: 395, Loss: 4.670\n",
      "Epoch: 396, Loss: 4.560\n",
      "Epoch: 397, Loss: 3.583\n",
      "Epoch: 398, Loss: 4.396\n",
      "Epoch: 399, Loss: 3.944\n",
      "Epoch: 400, Loss: 4.463\n",
      "Epoch: 401, Loss: 5.001\n",
      "Epoch: 402, Loss: 4.054\n",
      "Epoch: 403, Loss: 4.408\n",
      "Epoch: 404, Loss: 5.118\n",
      "Epoch: 405, Loss: 4.597\n",
      "Epoch: 406, Loss: 4.456\n",
      "Epoch: 407, Loss: 4.494\n",
      "Epoch: 408, Loss: 4.044\n",
      "Epoch: 409, Loss: 4.269\n",
      "Epoch: 410, Loss: 4.164\n",
      "Epoch: 411, Loss: 4.198\n",
      "Epoch: 412, Loss: 4.827\n",
      "Epoch: 413, Loss: 4.275\n",
      "Epoch: 414, Loss: 3.905\n",
      "Epoch: 415, Loss: 4.264\n",
      "Epoch: 416, Loss: 4.726\n",
      "Epoch: 417, Loss: 5.154\n",
      "Epoch: 418, Loss: 4.776\n",
      "Epoch: 419, Loss: 4.757\n",
      "Epoch: 420, Loss: 3.974\n",
      "Epoch: 421, Loss: 4.447\n",
      "Epoch: 422, Loss: 4.389\n",
      "Epoch: 423, Loss: 4.022\n",
      "Epoch: 424, Loss: 4.842\n",
      "Epoch: 425, Loss: 4.400\n",
      "Epoch: 426, Loss: 4.646\n",
      "Epoch: 427, Loss: 3.786\n",
      "Epoch: 428, Loss: 4.243\n",
      "Epoch: 429, Loss: 4.467\n",
      "Epoch: 430, Loss: 3.984\n",
      "Epoch: 431, Loss: 4.225\n",
      "Epoch: 432, Loss: 4.599\n",
      "Epoch: 433, Loss: 3.610\n",
      "Epoch: 434, Loss: 4.553\n",
      "Epoch: 435, Loss: 4.368\n",
      "Epoch: 436, Loss: 4.435\n",
      "Epoch: 437, Loss: 4.763\n",
      "Epoch: 438, Loss: 4.359\n",
      "Epoch: 439, Loss: 4.038\n",
      "Epoch: 440, Loss: 4.482\n",
      "Epoch: 441, Loss: 3.820\n",
      "Epoch: 442, Loss: 4.427\n",
      "Epoch: 443, Loss: 4.167\n",
      "Epoch: 444, Loss: 4.131\n",
      "Epoch: 445, Loss: 4.237\n",
      "Epoch: 446, Loss: 3.941\n",
      "Epoch: 447, Loss: 4.226\n",
      "Epoch: 448, Loss: 3.922\n",
      "Epoch: 449, Loss: 4.243\n",
      "Epoch: 450, Loss: 5.134\n",
      "Epoch: 451, Loss: 4.118\n",
      "Epoch: 452, Loss: 4.615\n",
      "Epoch: 453, Loss: 4.420\n",
      "Epoch: 454, Loss: 4.724\n",
      "Epoch: 455, Loss: 4.352\n",
      "Epoch: 456, Loss: 3.994\n",
      "Epoch: 457, Loss: 4.580\n",
      "Epoch: 458, Loss: 4.822\n",
      "Epoch: 459, Loss: 4.486\n",
      "Epoch: 460, Loss: 4.050\n",
      "Epoch: 461, Loss: 3.870\n",
      "Epoch: 462, Loss: 4.821\n",
      "Epoch: 463, Loss: 4.083\n",
      "Epoch: 464, Loss: 4.120\n",
      "Epoch: 465, Loss: 5.064\n",
      "Epoch: 466, Loss: 3.823\n",
      "Epoch: 467, Loss: 4.239\n",
      "Epoch: 468, Loss: 4.911\n",
      "Epoch: 469, Loss: 4.931\n",
      "Epoch: 470, Loss: 3.952\n",
      "Epoch: 471, Loss: 4.785\n",
      "Epoch: 472, Loss: 4.427\n",
      "Epoch: 473, Loss: 4.085\n",
      "Epoch: 474, Loss: 3.594\n",
      "Epoch: 475, Loss: 4.288\n",
      "Epoch: 476, Loss: 4.916\n",
      "Epoch: 477, Loss: 4.228\n",
      "Epoch: 478, Loss: 3.914\n",
      "Epoch: 479, Loss: 4.457\n",
      "Epoch: 480, Loss: 4.397\n",
      "Epoch: 481, Loss: 3.913\n",
      "Epoch: 482, Loss: 3.965\n",
      "Epoch: 483, Loss: 4.148\n",
      "Epoch: 484, Loss: 4.484\n",
      "Epoch: 485, Loss: 4.337\n",
      "Epoch: 486, Loss: 4.604\n",
      "Epoch: 487, Loss: 4.614\n",
      "Epoch: 488, Loss: 4.116\n",
      "Epoch: 489, Loss: 4.429\n",
      "Epoch: 490, Loss: 4.906\n",
      "Epoch: 491, Loss: 4.661\n",
      "Epoch: 492, Loss: 4.994\n",
      "Epoch: 493, Loss: 4.406\n",
      "Epoch: 494, Loss: 3.973\n",
      "Epoch: 495, Loss: 4.384\n",
      "Epoch: 496, Loss: 3.511\n",
      "Epoch: 497, Loss: 4.714\n",
      "Epoch: 498, Loss: 4.485\n",
      "Epoch: 499, Loss: 5.084\n",
      "Epoch: 500, Loss: 4.537\n",
      "Epoch: 501, Loss: 3.798\n",
      "Epoch: 502, Loss: 4.325\n",
      "Epoch: 503, Loss: 4.443\n",
      "Epoch: 504, Loss: 3.904\n",
      "Epoch: 505, Loss: 4.370\n",
      "Epoch: 506, Loss: 4.210\n",
      "Epoch: 507, Loss: 3.881\n",
      "Epoch: 508, Loss: 4.593\n",
      "Epoch: 509, Loss: 4.087\n",
      "Epoch: 510, Loss: 4.355\n",
      "Epoch: 511, Loss: 4.386\n",
      "Epoch: 512, Loss: 3.827\n",
      "Epoch: 513, Loss: 4.225\n",
      "Epoch: 514, Loss: 3.776\n",
      "Epoch: 515, Loss: 4.899\n",
      "Epoch: 516, Loss: 4.235\n",
      "Epoch: 517, Loss: 4.627\n",
      "Epoch: 518, Loss: 4.679\n",
      "Epoch: 519, Loss: 3.959\n",
      "Epoch: 520, Loss: 3.873\n",
      "Epoch: 521, Loss: 3.969\n",
      "Epoch: 522, Loss: 4.355\n",
      "Epoch: 523, Loss: 4.451\n",
      "Epoch: 524, Loss: 4.044\n",
      "Epoch: 525, Loss: 4.318\n",
      "Epoch: 526, Loss: 4.238\n",
      "Epoch: 527, Loss: 4.165\n",
      "Epoch: 528, Loss: 3.946\n",
      "Epoch: 529, Loss: 4.671\n",
      "Epoch: 530, Loss: 4.689\n",
      "Epoch: 531, Loss: 3.909\n",
      "Epoch: 532, Loss: 3.729\n",
      "Epoch: 533, Loss: 4.342\n",
      "Epoch: 534, Loss: 3.899\n",
      "Epoch: 535, Loss: 4.374\n",
      "Epoch: 536, Loss: 4.589\n",
      "Epoch: 537, Loss: 4.423\n",
      "Epoch: 538, Loss: 4.500\n",
      "Epoch: 539, Loss: 3.610\n",
      "Epoch: 540, Loss: 4.022\n",
      "Epoch: 541, Loss: 4.285\n",
      "Epoch: 542, Loss: 4.354\n",
      "Epoch: 543, Loss: 4.477\n",
      "Epoch: 544, Loss: 4.250\n",
      "Epoch: 545, Loss: 4.515\n",
      "Epoch: 546, Loss: 4.790\n",
      "Epoch: 547, Loss: 4.560\n",
      "Epoch: 548, Loss: 3.935\n",
      "Epoch: 549, Loss: 4.102\n",
      "Epoch: 550, Loss: 4.555\n",
      "Epoch: 551, Loss: 4.251\n",
      "Epoch: 552, Loss: 3.547\n",
      "Epoch: 553, Loss: 4.139\n",
      "Epoch: 554, Loss: 4.190\n",
      "Epoch: 555, Loss: 3.901\n",
      "Epoch: 556, Loss: 4.148\n",
      "Epoch: 557, Loss: 4.078\n",
      "Epoch: 558, Loss: 4.049\n",
      "Epoch: 559, Loss: 4.194\n",
      "Epoch: 560, Loss: 3.492\n",
      "Epoch: 561, Loss: 3.841\n",
      "Epoch: 562, Loss: 4.127\n",
      "Epoch: 563, Loss: 4.281\n",
      "Epoch: 564, Loss: 4.416\n",
      "Epoch: 565, Loss: 4.201\n",
      "Epoch: 566, Loss: 4.208\n",
      "Epoch: 567, Loss: 4.443\n",
      "Epoch: 568, Loss: 4.001\n",
      "Epoch: 569, Loss: 3.888\n",
      "Epoch: 570, Loss: 3.852\n",
      "Epoch: 571, Loss: 4.961\n",
      "Epoch: 572, Loss: 4.224\n",
      "Epoch: 573, Loss: 4.661\n",
      "Epoch: 574, Loss: 3.586\n",
      "Epoch: 575, Loss: 3.919\n",
      "Epoch: 576, Loss: 4.372\n",
      "Epoch: 577, Loss: 5.021\n",
      "Epoch: 578, Loss: 3.733\n",
      "Epoch: 579, Loss: 4.127\n",
      "Epoch: 580, Loss: 4.208\n",
      "Epoch: 581, Loss: 4.411\n",
      "Epoch: 582, Loss: 4.362\n",
      "Epoch: 583, Loss: 4.061\n",
      "Epoch: 584, Loss: 4.277\n",
      "Epoch: 585, Loss: 4.756\n",
      "Epoch: 586, Loss: 4.737\n",
      "Epoch: 587, Loss: 4.160\n",
      "Epoch: 588, Loss: 4.557\n",
      "Epoch: 589, Loss: 4.853\n",
      "Epoch: 590, Loss: 4.411\n",
      "Epoch: 591, Loss: 4.253\n",
      "Epoch: 592, Loss: 3.976\n",
      "Epoch: 593, Loss: 3.887\n",
      "Epoch: 594, Loss: 3.780\n",
      "Epoch: 595, Loss: 3.883\n",
      "Epoch: 596, Loss: 4.426\n",
      "Epoch: 597, Loss: 4.134\n",
      "Epoch: 598, Loss: 3.888\n",
      "Epoch: 599, Loss: 4.449\n",
      "Epoch: 600, Loss: 4.084\n",
      "Epoch: 601, Loss: 4.067\n",
      "Epoch: 602, Loss: 4.629\n",
      "Epoch: 603, Loss: 4.067\n",
      "Epoch: 604, Loss: 4.143\n",
      "Epoch: 605, Loss: 4.638\n",
      "Epoch: 606, Loss: 3.959\n",
      "Epoch: 607, Loss: 4.151\n",
      "Epoch: 608, Loss: 3.926\n",
      "Epoch: 609, Loss: 3.824\n",
      "Epoch: 610, Loss: 3.650\n",
      "Epoch: 611, Loss: 3.706\n",
      "Epoch: 612, Loss: 4.733\n",
      "Epoch: 613, Loss: 4.241\n",
      "Epoch: 614, Loss: 4.221\n",
      "Epoch: 615, Loss: 3.785\n",
      "Epoch: 616, Loss: 4.268\n",
      "Epoch: 617, Loss: 4.007\n",
      "Epoch: 618, Loss: 3.525\n",
      "Epoch: 619, Loss: 4.342\n",
      "Epoch: 620, Loss: 3.953\n",
      "Epoch: 621, Loss: 3.907\n",
      "Epoch: 622, Loss: 4.156\n",
      "Epoch: 623, Loss: 4.394\n",
      "Epoch: 624, Loss: 4.185\n",
      "Epoch: 625, Loss: 5.169\n",
      "Epoch: 626, Loss: 3.689\n",
      "Epoch: 627, Loss: 4.828\n",
      "Epoch: 628, Loss: 4.242\n",
      "Epoch: 629, Loss: 4.124\n",
      "Epoch: 630, Loss: 4.357\n",
      "Epoch: 631, Loss: 4.189\n",
      "Epoch: 632, Loss: 4.551\n",
      "Epoch: 633, Loss: 4.094\n",
      "Epoch: 634, Loss: 4.563\n",
      "Epoch: 635, Loss: 3.837\n",
      "Epoch: 636, Loss: 4.672\n",
      "Epoch: 637, Loss: 3.987\n",
      "Epoch: 638, Loss: 3.610\n",
      "Epoch: 639, Loss: 4.232\n",
      "Epoch: 640, Loss: 4.061\n",
      "Epoch: 641, Loss: 4.087\n",
      "Epoch: 642, Loss: 3.828\n",
      "Epoch: 643, Loss: 3.804\n",
      "Epoch: 644, Loss: 4.171\n",
      "Epoch: 645, Loss: 4.614\n",
      "Epoch: 646, Loss: 4.187\n",
      "Epoch: 647, Loss: 4.216\n",
      "Epoch: 648, Loss: 3.868\n",
      "Epoch: 649, Loss: 4.070\n",
      "Epoch: 650, Loss: 4.188\n",
      "Epoch: 651, Loss: 3.794\n",
      "Epoch: 652, Loss: 4.832\n",
      "Epoch: 653, Loss: 3.937\n",
      "Epoch: 654, Loss: 4.441\n",
      "Epoch: 655, Loss: 4.478\n",
      "Epoch: 656, Loss: 4.345\n",
      "Epoch: 657, Loss: 3.911\n",
      "Epoch: 658, Loss: 3.837\n",
      "Epoch: 659, Loss: 4.758\n",
      "Epoch: 660, Loss: 4.146\n",
      "Epoch: 661, Loss: 4.199\n",
      "Epoch: 662, Loss: 4.073\n",
      "Epoch: 663, Loss: 4.572\n",
      "Epoch: 664, Loss: 4.424\n",
      "Epoch: 665, Loss: 3.802\n",
      "Epoch: 666, Loss: 4.506\n",
      "Epoch: 667, Loss: 3.841\n",
      "Epoch: 668, Loss: 4.745\n",
      "Epoch: 669, Loss: 4.141\n",
      "Epoch: 670, Loss: 3.746\n",
      "Epoch: 671, Loss: 4.263\n",
      "Epoch: 672, Loss: 3.644\n",
      "Epoch: 673, Loss: 4.722\n",
      "Epoch: 674, Loss: 3.623\n",
      "Epoch: 675, Loss: 3.976\n",
      "Epoch: 676, Loss: 4.553\n",
      "Epoch: 677, Loss: 4.027\n",
      "Epoch: 678, Loss: 3.997\n",
      "Epoch: 679, Loss: 4.153\n",
      "Epoch: 680, Loss: 4.549\n",
      "Epoch: 681, Loss: 3.775\n",
      "Epoch: 682, Loss: 3.824\n",
      "Epoch: 683, Loss: 4.844\n",
      "Epoch: 684, Loss: 4.311\n",
      "Epoch: 685, Loss: 3.791\n",
      "Epoch: 686, Loss: 4.098\n",
      "Epoch: 687, Loss: 4.571\n",
      "Epoch: 688, Loss: 3.758\n",
      "Epoch: 689, Loss: 4.412\n",
      "Epoch: 690, Loss: 4.103\n",
      "Epoch: 691, Loss: 4.518\n",
      "Epoch: 692, Loss: 4.563\n",
      "Epoch: 693, Loss: 4.167\n",
      "Epoch: 694, Loss: 4.015\n",
      "Epoch: 695, Loss: 3.792\n",
      "Epoch: 696, Loss: 3.900\n",
      "Epoch: 697, Loss: 4.386\n",
      "Epoch: 698, Loss: 4.038\n",
      "Epoch: 699, Loss: 4.041\n",
      "Epoch: 700, Loss: 4.480\n",
      "Epoch: 701, Loss: 3.966\n",
      "Epoch: 702, Loss: 3.887\n",
      "Epoch: 703, Loss: 4.541\n",
      "Epoch: 704, Loss: 3.870\n",
      "Epoch: 705, Loss: 4.126\n",
      "Epoch: 706, Loss: 4.114\n",
      "Epoch: 707, Loss: 4.230\n",
      "Epoch: 708, Loss: 3.993\n",
      "Epoch: 709, Loss: 4.303\n",
      "Epoch: 710, Loss: 4.186\n",
      "Epoch: 711, Loss: 3.858\n",
      "Epoch: 712, Loss: 3.622\n",
      "Epoch: 713, Loss: 4.604\n",
      "Epoch: 714, Loss: 4.376\n",
      "Epoch: 715, Loss: 3.779\n",
      "Epoch: 716, Loss: 3.907\n",
      "Epoch: 717, Loss: 4.008\n",
      "Epoch: 718, Loss: 4.179\n",
      "Epoch: 719, Loss: 3.802\n",
      "Epoch: 720, Loss: 4.068\n",
      "Epoch: 721, Loss: 4.048\n",
      "Epoch: 722, Loss: 3.944\n",
      "Epoch: 723, Loss: 4.284\n",
      "Epoch: 724, Loss: 3.808\n",
      "Epoch: 725, Loss: 4.074\n",
      "Epoch: 726, Loss: 4.316\n",
      "Epoch: 727, Loss: 4.115\n",
      "Epoch: 728, Loss: 3.470\n",
      "Epoch: 729, Loss: 4.827\n",
      "Epoch: 730, Loss: 3.948\n",
      "Epoch: 731, Loss: 4.264\n",
      "Epoch: 732, Loss: 3.597\n",
      "Epoch: 733, Loss: 4.144\n",
      "Epoch: 734, Loss: 3.807\n",
      "Epoch: 735, Loss: 4.300\n",
      "Epoch: 736, Loss: 4.051\n",
      "Epoch: 737, Loss: 4.187\n",
      "Epoch: 738, Loss: 4.104\n",
      "Epoch: 739, Loss: 4.105\n",
      "Epoch: 740, Loss: 4.535\n",
      "Epoch: 741, Loss: 3.887\n",
      "Epoch: 742, Loss: 4.123\n",
      "Epoch: 743, Loss: 3.825\n",
      "Epoch: 744, Loss: 3.544\n",
      "Epoch: 745, Loss: 3.630\n",
      "Epoch: 746, Loss: 3.471\n",
      "Epoch: 747, Loss: 3.849\n",
      "Epoch: 748, Loss: 3.941\n",
      "Epoch: 749, Loss: 4.094\n",
      "Epoch: 750, Loss: 3.931\n",
      "Epoch: 751, Loss: 3.748\n",
      "Epoch: 752, Loss: 4.424\n",
      "Epoch: 753, Loss: 3.645\n",
      "Epoch: 754, Loss: 4.226\n",
      "Epoch: 755, Loss: 4.075\n",
      "Epoch: 756, Loss: 4.147\n",
      "Epoch: 757, Loss: 3.786\n",
      "Epoch: 758, Loss: 4.310\n",
      "Epoch: 759, Loss: 4.150\n",
      "Epoch: 760, Loss: 3.802\n",
      "Epoch: 761, Loss: 3.897\n",
      "Epoch: 762, Loss: 3.698\n",
      "Epoch: 763, Loss: 3.968\n",
      "Epoch: 764, Loss: 4.176\n",
      "Epoch: 765, Loss: 3.989\n",
      "Epoch: 766, Loss: 3.933\n",
      "Epoch: 767, Loss: 3.923\n",
      "Epoch: 768, Loss: 3.865\n",
      "Epoch: 769, Loss: 4.373\n",
      "Epoch: 770, Loss: 4.032\n",
      "Epoch: 771, Loss: 3.906\n",
      "Epoch: 772, Loss: 4.421\n",
      "Epoch: 773, Loss: 3.897\n",
      "Epoch: 774, Loss: 4.658\n",
      "Epoch: 775, Loss: 4.404\n",
      "Epoch: 776, Loss: 4.175\n",
      "Epoch: 777, Loss: 4.264\n",
      "Epoch: 778, Loss: 3.802\n",
      "Epoch: 779, Loss: 3.493\n",
      "Epoch: 780, Loss: 3.912\n",
      "Epoch: 781, Loss: 4.405\n",
      "Epoch: 782, Loss: 4.170\n",
      "Epoch: 783, Loss: 3.495\n",
      "Epoch: 784, Loss: 3.815\n",
      "Epoch: 785, Loss: 3.973\n",
      "Epoch: 786, Loss: 4.646\n",
      "Epoch: 787, Loss: 4.060\n",
      "Epoch: 788, Loss: 3.755\n",
      "Epoch: 789, Loss: 3.609\n",
      "Epoch: 790, Loss: 4.240\n",
      "Epoch: 791, Loss: 4.286\n",
      "Epoch: 792, Loss: 4.005\n",
      "Epoch: 793, Loss: 4.140\n",
      "Epoch: 794, Loss: 3.888\n",
      "Epoch: 795, Loss: 3.818\n",
      "Epoch: 796, Loss: 3.960\n",
      "Epoch: 797, Loss: 3.754\n",
      "Epoch: 798, Loss: 3.517\n",
      "Epoch: 799, Loss: 3.837\n",
      "Epoch: 800, Loss: 4.321\n",
      "Epoch: 801, Loss: 3.854\n",
      "Epoch: 802, Loss: 4.076\n",
      "Epoch: 803, Loss: 4.248\n",
      "Epoch: 804, Loss: 3.739\n",
      "Epoch: 805, Loss: 4.341\n",
      "Epoch: 806, Loss: 4.073\n",
      "Epoch: 807, Loss: 4.166\n",
      "Epoch: 808, Loss: 3.937\n",
      "Epoch: 809, Loss: 4.109\n",
      "Epoch: 810, Loss: 4.454\n",
      "Epoch: 811, Loss: 4.008\n",
      "Epoch: 812, Loss: 4.358\n",
      "Epoch: 813, Loss: 4.133\n",
      "Epoch: 814, Loss: 4.162\n",
      "Epoch: 815, Loss: 3.960\n",
      "Epoch: 816, Loss: 3.587\n",
      "Epoch: 817, Loss: 3.801\n",
      "Epoch: 818, Loss: 3.993\n",
      "Epoch: 819, Loss: 3.677\n",
      "Epoch: 820, Loss: 3.865\n",
      "Epoch: 821, Loss: 3.938\n",
      "Epoch: 822, Loss: 3.715\n",
      "Epoch: 823, Loss: 4.506\n",
      "Epoch: 824, Loss: 4.286\n",
      "Epoch: 825, Loss: 4.203\n",
      "Epoch: 826, Loss: 4.129\n",
      "Epoch: 827, Loss: 4.349\n",
      "Epoch: 828, Loss: 3.719\n",
      "Epoch: 829, Loss: 3.498\n",
      "Epoch: 830, Loss: 3.699\n",
      "Epoch: 831, Loss: 4.131\n",
      "Epoch: 832, Loss: 3.998\n",
      "Epoch: 833, Loss: 4.207\n",
      "Epoch: 834, Loss: 4.057\n",
      "Epoch: 835, Loss: 3.937\n",
      "Epoch: 836, Loss: 4.268\n",
      "Epoch: 837, Loss: 4.036\n",
      "Epoch: 838, Loss: 3.760\n",
      "Epoch: 839, Loss: 4.076\n",
      "Epoch: 840, Loss: 4.027\n",
      "Epoch: 841, Loss: 3.999\n",
      "Epoch: 842, Loss: 3.727\n",
      "Epoch: 843, Loss: 3.880\n",
      "Epoch: 844, Loss: 3.644\n",
      "Epoch: 845, Loss: 3.559\n",
      "Epoch: 846, Loss: 4.033\n",
      "Epoch: 847, Loss: 3.874\n",
      "Epoch: 848, Loss: 3.378\n",
      "Epoch: 849, Loss: 3.978\n",
      "Epoch: 850, Loss: 3.652\n",
      "Epoch: 851, Loss: 3.913\n",
      "Epoch: 852, Loss: 4.025\n",
      "Epoch: 853, Loss: 3.552\n",
      "Epoch: 854, Loss: 4.047\n",
      "Epoch: 855, Loss: 3.697\n",
      "Epoch: 856, Loss: 4.059\n",
      "Epoch: 857, Loss: 3.861\n",
      "Epoch: 858, Loss: 3.638\n",
      "Epoch: 859, Loss: 3.724\n",
      "Epoch: 860, Loss: 3.662\n",
      "Epoch: 861, Loss: 3.528\n",
      "Epoch: 862, Loss: 4.540\n",
      "Epoch: 863, Loss: 3.933\n",
      "Epoch: 864, Loss: 3.685\n",
      "Epoch: 865, Loss: 3.924\n",
      "Epoch: 866, Loss: 3.781\n",
      "Epoch: 867, Loss: 3.738\n",
      "Epoch: 868, Loss: 3.894\n",
      "Epoch: 869, Loss: 3.781\n",
      "Epoch: 870, Loss: 3.702\n",
      "Epoch: 871, Loss: 3.989\n",
      "Epoch: 872, Loss: 4.102\n",
      "Epoch: 873, Loss: 3.451\n",
      "Epoch: 874, Loss: 3.456\n",
      "Epoch: 875, Loss: 4.212\n",
      "Epoch: 876, Loss: 3.780\n",
      "Epoch: 877, Loss: 4.124\n",
      "Epoch: 878, Loss: 3.947\n",
      "Epoch: 879, Loss: 3.595\n",
      "Epoch: 880, Loss: 4.381\n",
      "Epoch: 881, Loss: 3.641\n",
      "Epoch: 882, Loss: 3.472\n",
      "Epoch: 883, Loss: 3.876\n",
      "Epoch: 884, Loss: 4.268\n",
      "Epoch: 885, Loss: 4.059\n",
      "Epoch: 886, Loss: 3.571\n",
      "Epoch: 887, Loss: 3.816\n",
      "Epoch: 888, Loss: 3.874\n",
      "Epoch: 889, Loss: 4.228\n",
      "Epoch: 890, Loss: 4.052\n",
      "Epoch: 891, Loss: 3.688\n",
      "Epoch: 892, Loss: 4.262\n",
      "Epoch: 893, Loss: 3.747\n",
      "Epoch: 894, Loss: 3.706\n",
      "Epoch: 895, Loss: 3.655\n",
      "Epoch: 896, Loss: 3.818\n",
      "Epoch: 897, Loss: 4.007\n",
      "Epoch: 898, Loss: 4.045\n",
      "Epoch: 899, Loss: 3.959\n",
      "Epoch: 900, Loss: 4.394\n",
      "Epoch: 901, Loss: 3.936\n",
      "Epoch: 902, Loss: 4.798\n",
      "Epoch: 903, Loss: 4.063\n",
      "Epoch: 904, Loss: 3.601\n",
      "Epoch: 905, Loss: 3.934\n",
      "Epoch: 906, Loss: 3.996\n",
      "Epoch: 907, Loss: 3.582\n",
      "Epoch: 908, Loss: 3.732\n",
      "Epoch: 909, Loss: 4.218\n",
      "Epoch: 910, Loss: 4.231\n",
      "Epoch: 911, Loss: 3.654\n",
      "Epoch: 912, Loss: 4.068\n",
      "Epoch: 913, Loss: 4.241\n",
      "Epoch: 914, Loss: 4.039\n",
      "Epoch: 915, Loss: 4.144\n",
      "Epoch: 916, Loss: 3.854\n",
      "Epoch: 917, Loss: 3.568\n",
      "Epoch: 918, Loss: 4.063\n",
      "Epoch: 919, Loss: 4.082\n",
      "Epoch: 920, Loss: 3.772\n",
      "Epoch: 921, Loss: 3.969\n",
      "Epoch: 922, Loss: 3.639\n",
      "Epoch: 923, Loss: 4.546\n",
      "Epoch: 924, Loss: 3.856\n",
      "Epoch: 925, Loss: 3.732\n",
      "Epoch: 926, Loss: 3.831\n",
      "Epoch: 927, Loss: 4.223\n",
      "Epoch: 928, Loss: 4.008\n",
      "Epoch: 929, Loss: 3.597\n",
      "Epoch: 930, Loss: 4.054\n",
      "Epoch: 931, Loss: 3.760\n",
      "Epoch: 932, Loss: 4.162\n",
      "Epoch: 933, Loss: 3.727\n",
      "Epoch: 934, Loss: 3.697\n",
      "Epoch: 935, Loss: 3.932\n",
      "Epoch: 936, Loss: 4.296\n",
      "Epoch: 937, Loss: 3.742\n",
      "Epoch: 938, Loss: 4.047\n",
      "Epoch: 939, Loss: 3.844\n",
      "Epoch: 940, Loss: 4.372\n",
      "Epoch: 941, Loss: 3.935\n",
      "Epoch: 942, Loss: 3.850\n",
      "Epoch: 943, Loss: 4.185\n",
      "Epoch: 944, Loss: 4.126\n",
      "Epoch: 945, Loss: 4.522\n",
      "Epoch: 946, Loss: 3.911\n",
      "Epoch: 947, Loss: 4.088\n",
      "Epoch: 948, Loss: 4.035\n",
      "Epoch: 949, Loss: 3.894\n",
      "Epoch: 950, Loss: 4.080\n",
      "Epoch: 951, Loss: 3.869\n",
      "Epoch: 952, Loss: 4.231\n",
      "Epoch: 953, Loss: 3.544\n",
      "Epoch: 954, Loss: 3.669\n",
      "Epoch: 955, Loss: 4.280\n",
      "Epoch: 956, Loss: 4.116\n",
      "Epoch: 957, Loss: 4.354\n",
      "Epoch: 958, Loss: 4.033\n",
      "Epoch: 959, Loss: 3.611\n",
      "Epoch: 960, Loss: 4.117\n",
      "Epoch: 961, Loss: 3.852\n",
      "Epoch: 962, Loss: 4.473\n",
      "Epoch: 963, Loss: 4.462\n",
      "Epoch: 964, Loss: 3.565\n",
      "Epoch: 965, Loss: 3.585\n",
      "Epoch: 966, Loss: 4.296\n",
      "Epoch: 967, Loss: 3.559\n",
      "Epoch: 968, Loss: 3.736\n",
      "Epoch: 969, Loss: 4.370\n",
      "Epoch: 970, Loss: 3.620\n",
      "Epoch: 971, Loss: 4.290\n",
      "Epoch: 972, Loss: 4.644\n",
      "Epoch: 973, Loss: 3.667\n",
      "Epoch: 974, Loss: 4.309\n",
      "Epoch: 975, Loss: 4.004\n",
      "Epoch: 976, Loss: 3.636\n",
      "Epoch: 977, Loss: 3.307\n",
      "Epoch: 978, Loss: 4.062\n",
      "Epoch: 979, Loss: 4.250\n",
      "Epoch: 980, Loss: 3.809\n",
      "Epoch: 981, Loss: 4.211\n",
      "Epoch: 982, Loss: 4.111\n",
      "Epoch: 983, Loss: 4.097\n",
      "Epoch: 984, Loss: 3.619\n",
      "Epoch: 985, Loss: 3.647\n",
      "Epoch: 986, Loss: 3.646\n",
      "Epoch: 987, Loss: 3.997\n",
      "Epoch: 988, Loss: 3.393\n",
      "Epoch: 989, Loss: 4.213\n",
      "Epoch: 990, Loss: 3.623\n",
      "Epoch: 991, Loss: 3.942\n",
      "Epoch: 992, Loss: 3.849\n",
      "Epoch: 993, Loss: 3.730\n",
      "Epoch: 994, Loss: 4.548\n",
      "Epoch: 995, Loss: 3.633\n",
      "Epoch: 996, Loss: 3.961\n",
      "Epoch: 997, Loss: 3.850\n",
      "Epoch: 998, Loss: 4.335\n",
      "Epoch: 999, Loss: 4.108\n",
      "Epoch: 1000, Loss: 3.431\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            self.gradients[self] += n.gradients[self]\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Sum the partial with respect to the input over all the outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` Nodes representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    # Performs SGD\n",
    "    #\n",
    "    # Loop over the trainables\n",
    "    for t in trainables:\n",
    "        # Change the trainable's value by subtracting the learning rate\n",
    "        # multiplied by the partial of the cost with respect to this\n",
    "        # trainable.\n",
    "        partial = t.gradients[t]\n",
    "        t.value -= learning_rate * partial\n",
    "\n",
    "\n",
    "##-------\n",
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "#import numpy as np\n",
    "#from sklearn.datasets import load_boston\n",
    "#from sklearn.utils import shuffle, resample\n",
    "#from miniflow import *\n",
    "\n",
    "# Load data\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 1000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
